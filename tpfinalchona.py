# -*- coding: utf-8 -*-
"""tpfinalchona.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cGl66xMuD2r0PFwdFhv9ZZKKMu5RJOeS
"""

!pip install tensorflow keras numpy mnist matplotlib
#Catalogar dígitos escritos a mano usando Python y redes neuronales artificiales

#importa  packages / dependecnies
import numpy as пр
import mnist #Get data set from
import matplotlib.pyplot as plt #Graph
from keras.models import Sequential #ANN architecture
from keras.layers import Dense #The layers in the ANN
from keras.utils import to_categorical

#carga la data set
train_images = mnist.train_images()#training data images
train_labels = mnist.train_labels() #training data labels
test_images = mnist.test_images () #training data images
test_labels = mnist. test_labels() #training data labels

#normaliza todos los valores de los pixeles entre o y 1 
# [-0.5 و 0.5] para hacer nuestra red mas facil de entranar
train_images = (train_images/255) - 0.5
test_images = (test_images/255)- 0.5
#redimensiona las dimensiones de las imagenes
train_images = train_images.reshape((-1,784))
test_images = test_images.reshape((-1,784))
#Print la forma de las imagenes de entrenamiento y de testeo
print(train_images.shape)#60.000 filas y 784 columnas
print(test_images.shape)#10.000 filas y 784 columnas

#creamos el modelo
# 3 layers, 2 layers con 64 neuronas y la funcion relu
# 1 layer con 10 neuronas y la funcion softmax
model = Sequential()
model.add( Dense (64, activation='relu', input_dim=784))
model.add( Dense (64, activation='relu'))
model.add(Dense(10, activation='softmax'))

#Compilamos el modelo
#La función de pérdida mide qué tan bien se desempeñó el modelo en el entrenamiento 
#para mejorar la performance usamos este optimizador
model. compile(
   optimizer= 'adam',
     loss = 'categorical_crossentropy', #(cantidad de clases mayores a 2) I
    metrics =  ['accuracy']
)

#entrenamos el modelo
model.fit(
    train_images,
    to_categorical(train_labels),#Ex. 2 se espera[0, 0 ,1,0,0,0,0,0]
    epochs = 5, #el número de interacciones en la dataset para entrenar
    batch_size=32 #grupo chiquito de datos que usa para entrenar la guia
)

#evaluamos el modelo
model.evaluate(
    train_images,
    to_categorical(train_labels)
)

#model.save_weights('model.h5')

#predicciones de las primeras 5(numero que le pongas) imagenes de testeo
predictions = model.predict(test_images[:100])
#print las predicciones de nuestor modelos
print(пр.argmax(predictions, axis = 1))
print(test_labels[:100])

for i in range(0,5):
  first_image = test_images[i]
  first_image = пр.array(first_image, dtype ='float')
  pixels = first_image.reshape((28,28))
  plt.imshow(pixels, cmap='gray')
  plt.show()